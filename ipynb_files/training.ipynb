{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "#WordNetLemmatizer reduce the word to stem so that it wont loose performance\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "intents = json.loads(open('intents.json').read())\n",
    "#reading the json file as text\n",
    "\n",
    "\n",
    "#create 3 empty lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "\n",
    "''' \n",
    "How it works?:\n",
    "\n",
    "Look inside the intents in the json file and through the patterns. Then,\n",
    "append the words into the word list.\n",
    "'''\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize = splits up sentences into words\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        #(word_list) is a tuple. Tuple: stores multiple items into a single variable\n",
    "        #(word_list) belongs to the category intent['tag']\n",
    "        documents.append((word_list, intent['tag']))\n",
    "\n",
    "        #check if the class is in the classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "max_length = max(len(x) for x, y in training)\n",
    "\n",
    "for i, (x, y) in enumerate(training):\n",
    "    if len(x) < max_length: \n",
    "        training[i] = (x + [0] * (max_length - len(x)), y)\n",
    "    if len(y) < max_length: \n",
    "        training[i] = (y + [0] * (max_length - len(y)), x)\n",
    "  \n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "model = Sequential() #sequential model\n",
    "#              128 = Neurons  input_shape dependant on  the size of the training data of train_x\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.model')\n",
    "'''\n",
    "\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs = 200, batch_size=5, verbose=1)\n",
    "model.save('chatbotmodel.h5', hist) #save the training data into a h5 file... I think?\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
